{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c2677-ea98-4340-b6fc-2244f59eb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different from the version: Combat_function_CW_v1\n",
    "# delete some comments\n",
    "# seluse: Combat_function_CW_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdd4d7c-083b-40e3-9108-2d3f3b1825e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# ComBat function python version \n",
    "# reference: https://github.com/epigenelabs/pyComBat\n",
    "# revised some functions\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from math import exp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import mpmath as mp\n",
    "import pandas as pd\n",
    "\n",
    "from combat.exceptions import ConfoundingVariablesError\n",
    "\n",
    "#import unittest\n",
    "\n",
    "\n",
    "def model_matrix(info, intercept=True, drop_first=False): #  cw drop_first=True \n",
    "    \"\"\"Creates the model_matrix from batch list\n",
    "\n",
    "    Arguments:\n",
    "        info {list} -- list info with batch or covariates data\n",
    "        intercept {bool} -- boolean for intercept in model matrix\n",
    "\n",
    "    Returns:\n",
    "        matrix -- model matrix generate from batch list\n",
    "    \"\"\"\n",
    "    if not isinstance(info[0], list):\n",
    "        info = [info]\n",
    "    else:\n",
    "        info = info\n",
    "    info_dict = {}\n",
    "    for i in range(len(info)):\n",
    "        info_dict[f\"col{str(i)}\"] = list(map(str,info[i]))\n",
    "    df = pd.get_dummies(pd.DataFrame(info_dict), drop_first=drop_first, dtype=float) # cw  change the original drop_first=drop_first\n",
    "    if intercept:\n",
    "        df[\"intercept\"] = 1.0\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def all_1(list_of_elements):\n",
    "    \"\"\"checks if all elements in a list are 1s\n",
    "\n",
    "    Arguments:\n",
    "        list_of_elements {list} -- list of elements\n",
    "\n",
    "    Returns:\n",
    "        bool -- True iff all elements of the list are 1s\n",
    "    \"\"\"\n",
    "    return((list_of_elements == 1).all())\n",
    "\n",
    "\n",
    "def compute_prior(prior, delta_hat, mean_only):   # cw rewrite the function\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Arguments:\n",
    "        prior {char} -- 'a' or 'b' depending of the prior to be calculated\n",
    "        delta_hat {matrix} -- matrix of additive batch effect\n",
    "        mean_only {bool} -- True if mean_only selected\n",
    "\n",
    "    Returns:\n",
    "        float -- [the prior calculated (aprior or bprior)\n",
    "    \"\"\"\n",
    "    if mean_only:\n",
    "        return 1\n",
    "    m = np.mean(delta_hat)\n",
    "    s2 = np.var(delta_hat)\n",
    "    if prior == 'a':\n",
    "        return (2*s2+m*m)/s2\n",
    "    elif prior == 'b':\n",
    "        return (m*s2+m*m*m)/s2  \n",
    "\n",
    "\n",
    "def postmean(g_bar, d_star, t2_n, t2_n_g_hat):\n",
    "    \"\"\"estimates additive batch effect\n",
    "\n",
    "    Arguments:\n",
    "        g_bar {matrix} -- additive batch effect\n",
    "        d_star {matrix} -- multiplicative batch effect\n",
    "        t2_n {matrix} --\n",
    "        t2_n_g_hat {matrix} --\n",
    "\n",
    "    Returns:\n",
    "        matrix -- estimated additive batch effect\n",
    "    \"\"\"\n",
    "    return np.divide(t2_n_g_hat+d_star*g_bar, np.asarray(t2_n+d_star))\n",
    "\n",
    "\n",
    "def postvar(sum2, n, a, b):\n",
    "    \"\"\"estimates multiplicative batch effect\n",
    "\n",
    "    Arguments:\n",
    "        sum2 {vector} --\n",
    "        n {[type]} --\n",
    "        a {float} -- aprior\n",
    "        b {float} -- bprior\n",
    "\n",
    "    Returns:\n",
    "        matrix -- estimated multiplicative batch effect\n",
    "    \"\"\"\n",
    "    return(np.divide((np.multiply(0.5, sum2)+b), (np.multiply(0.5, n)+a-1)))\n",
    "    \n",
    "\n",
    "def it_sol(sdat, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001, exit_iteration=10e5):\n",
    "    \"\"\"iterative solution for Empirical Bayesian method\n",
    "\n",
    "    Arguments:\n",
    "        sdat {matrix} --\n",
    "        g_hat {matrix} -- average additive batch effect\n",
    "        d_hat {matrix} -- average multiplicative batch effect\n",
    "        g_bar {matrix} -- additive batch effect\n",
    "        t2 {matrix} --\n",
    "        a {float} -- aprior\n",
    "        b {float} -- bprior\n",
    "\n",
    "    Keyword Arguments:\n",
    "        conv {float} -- convergence criterion (default: {0.0001})\n",
    "        exit_iteration {float} -- maximum number of iterations before exit (default: {10e5})\n",
    "\n",
    "    Returns:\n",
    "        array list -- estimated additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "\n",
    "    n = [len(i) for i in np.asarray(sdat)]\n",
    "    t2_n = np.multiply(t2, n)\n",
    "    t2_n_g_hat = np.multiply(t2_n, g_hat)\n",
    "    g_old = np.ndarray.copy(g_hat)\n",
    "    d_old = np.ndarray.copy(d_hat)\n",
    "    change = 1\n",
    "    count = 0  # number of steps needed (for diagnostic only)\n",
    "    # convergence criteria, if new-old < conv, then stop\n",
    "    while (change > conv) and (count < exit_iteration):\n",
    "        g_new = postmean(g_bar, d_old, t2_n, t2_n_g_hat)  # updated additive batch effect\n",
    "        sum2 = np.sum(np.asarray(np.square(\n",
    "            sdat-np.outer(g_new[0][0], np.ones(np.ma.size(sdat, axis=1))))), axis=1)\n",
    "        d_new = postvar(sum2, n, a, b)  # updated multiplicative batch effect\n",
    "        change = max(np.amax(np.absolute(g_new-np.asarray(g_old))/np.asarray(g_old)), np.amax(\n",
    "            np.absolute(d_new-d_old)/d_old))  # maximum difference between new and old estimate\n",
    "        g_old = np.ndarray.copy(g_new)  # save value for g\n",
    "        d_old = np.ndarray.copy(d_new)  # save value for d\n",
    "        count += 1\n",
    "    adjust = np.asarray([g_new, d_new], dtype='object') #cw\n",
    "    return(adjust)  # remove parenthesis in returns\n",
    "\n",
    "# int_eprior - Monte Carlo integration function to find nonparametric adjustments\n",
    "# Johnson et al (Biostatistics 2007, supp.mat.) show that we can estimate the multiplicative and additive batch effects with an integral\n",
    "# This integral is numerically computed through Monte Carlo inegration (iterative method)\n",
    "\n",
    "\n",
    "def int_eprior(sdat, g_hat, d_hat, precision):\n",
    "    \"\"\" int_eprior - Monte Carlo integration function to find nonparametric adjustments\n",
    "        Johnson et al (Biostatistics 2007, supp.mat.) show that we can estimate the multiplicative and additive batch effects with an integral\n",
    "        This integral is numerically computed through Monte Carlo inegration (iterative method)\n",
    "\n",
    "    Arguments:\n",
    "        sdat {matrix} -- data matrix\n",
    "        g_hat {matrix} -- average additive batch effect\n",
    "        d_hat {matrix} -- average multiplicative batch effect\n",
    "        precision {float} -- level of precision for precision computing\n",
    "\n",
    "    Returns:\n",
    "        array list -- estimated additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "    g_star = []\n",
    "    d_star = []\n",
    "    # use this variable to only print error message once if approximation used\n",
    "    test_approximation = 0\n",
    "    for i in range(len(sdat)):\n",
    "        # additive batch effect\n",
    "        g = np.asarray(np.delete(np.transpose(g_hat), i))\n",
    "        # multiplicative batch effect\n",
    "        d = np.asarray(np.delete(np.transpose(d_hat), i))\n",
    "        x = np.asarray(np.transpose(sdat[i]))\n",
    "        n = len(x)\n",
    "        j = [1]*n\n",
    "        dat = np.repeat(x, len(np.transpose(g)), axis=1)\n",
    "        resid2 = np.square(dat-g)\n",
    "        sum2 = np.dot(np.transpose(resid2), j)\n",
    "        # /begin{handling high precision computing}\n",
    "        temp_2d = 2*d\n",
    "        if (precision == None):\n",
    "            LH = np.power(1/(np.pi*temp_2d), n/2)*np.exp(np.negative(sum2)/(temp_2d))\n",
    "\n",
    "        else:  # only if precision parameter informed\n",
    "            # increase the precision of the computing (if negative exponential too close to 0)\n",
    "            mp.dps = precision\n",
    "            buf_exp = np.array(list(map(mp.exp, np.negative(sum2)/(temp_2d))))\n",
    "            buf_pow = np.array(list(map(partial(mp.power, y=n/2), 1/(np.pi*temp_2d))))\n",
    "            #print(buf_exp.dtype, buf_pow.dtype)\n",
    "            LH = buf_pow*buf_exp  # likelihood\n",
    "        # /end{handling high precision computing}\n",
    "        LH = np.nan_to_num(LH)  # corrects NaNs in likelihood\n",
    "        if np.sum(LH) == 0 and test_approximation == 0:\n",
    "            test_approximation = 1  # this message won't appear again\n",
    "            print(\"###\\nValues too small, approximation applied to avoid division by 0.\\nPrecision mode can correct this problem, but increases computation time.\\n###\")\n",
    "\n",
    "        if np.sum(LH) == 0: # correction for LH full of 0.0\n",
    "            LH[LH == 0] = np.exp(-745)\n",
    "            g_star.append(np.sum(g*LH)/np.sum(LH))\n",
    "            d_star.append(np.sum(d*LH)/np.sum(LH))\n",
    "        else:\n",
    "            g_star.append(np.sum(g*LH)/np.sum(LH))\n",
    "            d_star.append(np.sum(d*LH)/np.sum(LH))\n",
    "    adjust = np.asarray([np.asarray(g_star), np.asarray(d_star)])\n",
    "    return(adjust)\n",
    "\n",
    "\n",
    "def param_fun(i, s_data, batches, mean_only, gamma_hat, gamma_bar, delta_hat, t2, a_prior, b_prior):\n",
    "    \"\"\"parametric estimation of batch effects\n",
    "\n",
    "    Arguments:\n",
    "        i {int} -- column index\n",
    "        s_data {matrix} --\n",
    "        batches {list list} -- list of list of batches' elements\n",
    "        mean_only {bool} -- True iff mean_only selected\n",
    "        gamma_hat {matrix} -- average additive batch effect\n",
    "        gamma_bar {matrix} -- estimated additive batch effect\n",
    "        delta_hat {matrix} -- average multiplicative batch effect\n",
    "        t2 {matrix} --\n",
    "        a_prior {float} -- aprior\n",
    "        b_prior {float} -- bprior\n",
    "\n",
    "    Returns:\n",
    "        array list -- estimated adjusted additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "    if mean_only:  # if mean_only, no need for complex method: batch effect is immediately calculated\n",
    "        t2_n = np.multiply(t2[i], 1)\n",
    "        t2_n_g_hat = np.multiply(t2_n, gamma_hat[i])\n",
    "        gamma_star = postmean(gamma_bar[i], 1, t2_n, t2_n_g_hat)  # additive batch effect\n",
    "        delta_star = [1]*len(s_data)  # multiplicative batch effect\n",
    "    else:  # if not(mean_only) then use it_solve\n",
    "        temp = it_sol(np.transpose(np.transpose(s_data)[\n",
    "                      batches[i]]), gamma_hat[i], delta_hat[i], gamma_bar[i], t2[i], a_prior[i], b_prior[i])\n",
    "        gamma_star = temp[0]  # additive batch effect\n",
    "        delta_star = temp[1]  # multiplicative batch effect\n",
    "    return [gamma_star, delta_star]\n",
    "\n",
    "def nonparam_fun(i, mean_only, delta_hat, s_data, batches, gamma_hat, precision):\n",
    "    \"\"\"non-parametric estimation\n",
    "\n",
    "    Arguments:\n",
    "        i {int} -- column index\n",
    "        mean_only {bool} -- True iff mean_only selected\n",
    "        delta_hat {matrix} -- estimated multiplicative batch effect\n",
    "        s_data {matrix} --\n",
    "        batches {list list} -- list of list of batches' elements\n",
    "        gamma_hat {matrix} -- estimated additive batch effect\n",
    "        precision {float} -- level of precision for precision computing\n",
    "\n",
    "    Returns:\n",
    "        array list -- estimated adjusted additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "    if mean_only:  # if mean only, change delta_hat to vector of 1s\n",
    "        delta_hat[i] = [1]*len(delta_hat[i])\n",
    "    # use int_eprior for non-parametric estimation\n",
    "    temp = int_eprior(np.transpose(np.transpose(s_data)[\n",
    "                      batches[i]]), gamma_hat[i], delta_hat[i], precision)\n",
    "    return [temp[0], temp[1]]\n",
    "\n",
    "############\n",
    "# pyComBat #\n",
    "############\n",
    "\n",
    "\n",
    "def check_mean_only(mean_only):\n",
    "    \"\"\"checks mean_only option\n",
    "\n",
    "    Arguments:\n",
    "        mean_only {boolean} -- user's choice about mean_only\n",
    "\n",
    "    Returns:\n",
    "        ()\n",
    "    \"\"\"\n",
    "    if mean_only == True:\n",
    "        print(\"Using mean only version\")\n",
    "\n",
    "\n",
    "def define_batchmod(batch):\n",
    "    \"\"\"generates model matrix\n",
    "\n",
    "    Arguments:\n",
    "        batch {list} -- list of batch id\n",
    "\n",
    "    Returns:\n",
    "        batchmod {matrix} -- model matrix for batches\n",
    "    \"\"\"\n",
    "    batchmod = model_matrix(list(batch), intercept=False, drop_first=False)\n",
    "    return(batchmod)\n",
    "\n",
    "\n",
    "def check_ref_batch(ref_batch, batch, batchmod):\n",
    "    \"\"\"check ref_batch option and treat it if needed\n",
    "\n",
    "    Arguments:\n",
    "        ref_batch -- the reference batch\n",
    "        batch {list} -- list of batch id\n",
    "        batchmod {matrix} -- model matrix related to batches\n",
    "\n",
    "    Returns:\n",
    "        ref {int} -- the index of the reference batch in the batch list\n",
    "        batchmod {matrix} -- updated model matrix related to batches, with reference\n",
    "    \"\"\"\n",
    "    if ref_batch is not None:\n",
    "        if ref_batch not in batch:\n",
    "            raise ValueError(\"Reference level ref.batch must be one of the levels of batch.\")\n",
    "        print(\"Using batch \"+str(ref_batch) +\n",
    "              \" as a reference batch.\")\n",
    "        # ref keeps in memory the columns concerned by the reference batch\n",
    "        ref = np.where(np.unique(batch) == ref_batch)[0][0]\n",
    "        #ref = ref + 1 # cw\n",
    "        # updates batchmod with reference\n",
    "        batchmod[:,ref] = 1\n",
    "        ref = ref + 1\n",
    "    else:\n",
    "        ref = None  # default settings\n",
    "    return(ref , batchmod)  # cw \n",
    " \n",
    "def treat_batches(batch):\n",
    "    \"\"\"treat batches\n",
    "\n",
    "    Arguments:\n",
    "        batch {list} -- batch list\n",
    "\n",
    "    Returns:\n",
    "        n_batch {int} -- number of batches\n",
    "        batches {int list} -- list of unique batches\n",
    "        n_batches {int list} -- list of batches lengths\n",
    "        n_array {int} -- total size of dataset\n",
    "    \"\"\"\n",
    "    batch = pd.Series(batch)\n",
    "    n_batch = len(np.unique(batch))  # number of batches\n",
    "    print(\"Found \"+str(n_batch)+\" batches.\")\n",
    "    batches = []  # list of lists, contains the list of position for each batch\n",
    "    for i in range(n_batch):\n",
    "        batches.append(np.where(batch == np.unique(batch)[i])[0].astype(np.int32))\n",
    "    n_batches = list(map(len, batches))\n",
    "    if 1 in n_batches:\n",
    "        #mean_only = True  # no variance if only one sample in a batch - mean_only has to be used\n",
    "        print(\"\\nOne batch has only one sample, try setting mean_only=True.\\n\")\n",
    "    n_array = sum(n_batches)\n",
    "    return(n_batch, batches, n_batches, n_array)\n",
    "\n",
    "\n",
    "def treat_covariates(batchmod, mod, ref, n_batch):   # cw recise version\n",
    "    \"\"\"treat covariates\n",
    "\n",
    "    Arguments:\n",
    "        batchmod {matrix} -- model matrix for batch\n",
    "        mod {matrix} -- model matrix for other covariates\n",
    "        ref {int} -- index of reference batch\n",
    "        n_batch {int} -- number of batches\n",
    "\n",
    "    Returns:\n",
    "        check {bool list} -- a list characterising all covariates\n",
    "        design {matrix} -- model matrix for all covariates, including batch\n",
    "    \"\"\"\n",
    "    # design matrix for sample conditions\n",
    "    if mod == []:\n",
    "        design = batchmod\n",
    "    else:\n",
    "        mod_matrix = model_matrix(mod, intercept=False) #cw\n",
    "        mod_matrix = mod_matrix[:,1:] # cw \n",
    "        design = np.concatenate((batchmod, mod_matrix), axis=1)\n",
    "    check = list(map(all_1, np.transpose(design)))\n",
    "    if ref is not None:  # if ref\n",
    "        check[ref-1] = False  # the reference batch is not considered a covariate # cw\n",
    "    design = design[:, ~np.array(check)]\n",
    "    design = np.transpose(design)\n",
    "\n",
    "    print(\"Adjusting for \"+str(len(design)-len(np.transpose(batchmod))) +\n",
    "          \" covariate(s) or covariate level(s).\")\n",
    "\n",
    "    # if matrix cannot be invertible, different cases #comment the below  cw\n",
    "    # if np.linalg.matrix_rank(design) < len(design):\n",
    "    #     if len(design) == n_batch + 1:  # case 1: covariate confunded with a batch\n",
    "    #         raise ConfoundingVariablesError(\"Covariate is confounded with batch. Try removing the covariates.\")\n",
    "    #     if len(design) > n_batch + 1:  # case 2: multiple covariates confunded with a batch\n",
    "    #         if np.linalg.matrix_rank(np.transpose(design)[:n_batch]) < len(design):\n",
    "    #             raise ConfoundingVariablesError(\"Confounded design. Try removing one or more covariates.\")\n",
    "    #         else:  # case 3: at least a covariate confunded with a batch\n",
    "    #             raise ConfoundingVariablesError(\"At least one covariate is confounded with batch. Try removing confounded covariates.\")\n",
    "    return(design)\n",
    "\n",
    "\n",
    "def check_NAs(dat):\n",
    "    \"\"\"check if NaNs - in theory, we construct the data without NAs\n",
    "\n",
    "    Arguments:\n",
    "        dat {matrix} -- the data matrix\n",
    "\n",
    "    Returns:\n",
    "        NAs {bool} -- boolean characterising the presence of NaNs in the data matrix\n",
    "    \"\"\"\n",
    "    # NAs = True in (np.isnan(dat))\n",
    "    NAs = np.isnan(np.sum(dat))  # Check if NaN exists\n",
    "    if NAs:\n",
    "        print(\"Found missing data values. Please remove all missing values before proceeding with pyComBat.\")\n",
    "    return(NAs)\n",
    "\n",
    "def calculate_mean_var(design, batches, ref, dat, NAs, n_batches, n_batch, n_array):  #cw revised\n",
    "    \"\"\" calculates the Normalisation factors\n",
    "\n",
    "    Arguments:\n",
    "        design {matrix} -- model matrix for all covariates\n",
    "        batches {int list} -- list of unique batches\n",
    "        ref {int} -- reference batch index\n",
    "        dat {matrix} -- data matrix\n",
    "        NAs {bool} -- presence of NaNs in the data matrix\n",
    "        n_batches {int list} -- list of batches lengths\n",
    "        n_array {int} -- total size of dataset\n",
    "\n",
    "    Returns:\n",
    "        B_hat {matrix} -- regression coefficients corresponding to the design matrix\n",
    "        grand_mean {matrix} -- Mean for each gene and each batch\n",
    "        var_pooled {matrix} -- Variance for each gene and each batch\n",
    "    \"\"\"\n",
    "    print(\"Standardizing Data across genes.\")\n",
    "    if not(NAs):  # NAs not supported\n",
    "        # B_hat is the vector of regression coefficients corresponding to the design matrix\n",
    "        B_hat = np.linalg.solve(np.dot(design, np.transpose(\n",
    "            design)), np.dot(design, np.transpose(dat)))\n",
    "\n",
    "    # Calculates the general mean\n",
    "    if ref is not None:\n",
    "        grand_mean = np.transpose(B_hat[ref-1]) # cw\n",
    "    else:\n",
    "        grand_mean = np.dot(np.transpose(\n",
    "            [i / n_array for i in n_batches]), B_hat[0:n_batch])\n",
    "    # Calculates the general variance\n",
    "    if not NAs:  # NAs not supported\n",
    "        if ref is not None:  # depending on ref batch\n",
    "            ref_dat = np.transpose(np.transpose(dat)[batches[ref-1]]) #cw\n",
    "            #var_pooled = np.dot(np.square(ref_dat - np.transpose(np.dot(np.transpose(\n",
    "            #    design)[batches[ref]], B_hat))), [1/n_batches[ref]]*n_batches[ref])\n",
    "            var_pooled = np.dot(np.square(ref_dat - np.transpose(np.dot(np.transpose(\n",
    "                design)[batches[ref-1]], B_hat))), [1/n_batches[ref-1]]*n_batches[ref-1])\n",
    "        else:\n",
    "            var_pooled = np.dot(np.square(\n",
    "                dat - np.transpose(np.dot(np.transpose(design), B_hat))), [1/n_array]*n_array)\n",
    "\n",
    "    return(B_hat, grand_mean, var_pooled)\n",
    "\n",
    "\n",
    "def calculate_stand_mean(grand_mean, n_array, design, n_batch, B_hat):\n",
    "    \"\"\" transform the format of the mean for substraction\n",
    "\n",
    "    Arguments:\n",
    "        grand_mean {matrix} -- Mean for each gene and each batch\n",
    "        n_array {int} -- total size of dataset\n",
    "        design {[type]} -- design matrix for all covariates including batch\n",
    "        n_batch {int} -- number of batches\n",
    "        B_hat {matrix} -- regression coefficients corresponding to the design matrix\n",
    "\n",
    "    Returns:\n",
    "        stand_mean {matrix} -- standardised mean\n",
    "    \"\"\"\n",
    "    stand_mean = np.dot(np.transpose(np.mat(grand_mean)), np.mat([1]*n_array))\n",
    "    # corrects the mean with design matrix information\n",
    "    if design is not None:\n",
    "        tmp = np.ndarray.copy(design)\n",
    "        tmp[0:n_batch] = 0\n",
    "        stand_mean = stand_mean + \\\n",
    "            np.transpose(np.dot(np.transpose(tmp), B_hat))\n",
    "    return(stand_mean)\n",
    "\n",
    "\n",
    "def standardise_data(dat, stand_mean, var_pooled, n_array):\n",
    "    \"\"\"standardise the data: substract mean and divide by variance\n",
    "\n",
    "    Arguments:\n",
    "        dat {matrix} -- data matrix\n",
    "        stand_mean {matrix} -- standardised mean\n",
    "        var_pooled {matrix} -- Variance for each gene and each batch\n",
    "        n_array {int} -- total size of dataset\n",
    "\n",
    "    Returns:\n",
    "        s_data {matrix} -- standardised data matrix\n",
    "    \"\"\"\n",
    "    s_data = (dat - stand_mean) / \\\n",
    "        np.dot(np.transpose(np.mat(np.sqrt(var_pooled))), np.mat([1]*n_array))\n",
    "    return(s_data)\n",
    "\n",
    "\n",
    "def fit_model(design, n_batch, s_data, batches, mean_only, par_prior, precision, ref, NAs):\n",
    "    print(\"Fitting L/S model and finding priors.\")\n",
    "\n",
    "    # fraction of design matrix related to batches\n",
    "    batch_design = design[0:n_batch]\n",
    "\n",
    "    if not NAs:  # CF SUPRA FOR NAs\n",
    "        # gamma_hat is the vector of additive batch effect\n",
    "        gamma_hat = np.linalg.solve(np.dot(batch_design, np.transpose(batch_design)),\n",
    "                                    np.dot(batch_design, np.transpose(s_data)))\n",
    "\n",
    "    delta_hat = []  # delta_hat is the vector of estimated multiplicative batch effect\n",
    "\n",
    "    if (mean_only):\n",
    "        # no variance if mean_only == True\n",
    "        delta_hat = [np.asarray([1]*len(s_data))] * len(batches)\n",
    "    else:\n",
    "        for i in batches:  # feed incrementally delta_hat\n",
    "            list_map = np.transpose(np.transpose(s_data)[i]).var(\n",
    "                axis=1)  # variance for each row\n",
    "            delta_hat.append(np.squeeze(np.asarray(list_map)))\n",
    "\n",
    "    gamma_bar = list(map(np.mean, gamma_hat))  # vector of means for gamma_hat\n",
    "    t2 = list(map(np.var, gamma_hat))  # vector of variances for gamma_hat\n",
    "\n",
    "    # calculates hyper priors for gamma (additive batch effect)\n",
    "    a_prior = list(\n",
    "        map(partial(compute_prior, 'a', mean_only=mean_only), delta_hat))\n",
    "    b_prior = list(\n",
    "        map(partial(compute_prior, 'b', mean_only=mean_only), delta_hat))\n",
    "\n",
    "    # initialise gamma and delta for parameters estimation\n",
    "    gamma_star = np.empty((n_batch, len(s_data)))\n",
    "    delta_star = np.empty((n_batch, len(s_data)))\n",
    "\n",
    "    if par_prior:\n",
    "        # use param_fun function for parametric adjustments (cf. function definition)\n",
    "        print(\"Finding parametric adjustments.\")\n",
    "        results = list(map(partial(param_fun,\n",
    "                                   s_data=s_data,\n",
    "                                   batches=batches,\n",
    "                                   mean_only=mean_only,\n",
    "                                   gamma_hat=gamma_hat,\n",
    "                                   gamma_bar=gamma_bar,\n",
    "                                   delta_hat=delta_hat,\n",
    "                                   t2=t2,\n",
    "                                   a_prior=a_prior,\n",
    "                                   b_prior=b_prior), range(n_batch)))\n",
    "    else:\n",
    "        # use nonparam_fun for non-parametric adjustments (cf. function definition)\n",
    "        print(\"Finding nonparametric adjustments\")\n",
    "        results = list(map(partial(nonparam_fun, mean_only=mean_only, delta_hat=delta_hat,\n",
    "                                   s_data=s_data, batches=batches, gamma_hat=gamma_hat, precision=precision), range(n_batch)))\n",
    "\n",
    "    for i in range(n_batch):  # store the results in gamma/delta_star\n",
    "        results_i = results[i]\n",
    "        gamma_star[i], delta_star[i] = results_i[0], results_i[1]\n",
    "\n",
    "    # update if reference batch (the reference batch is not supposed to be modified)\n",
    "    if ref is not None:\n",
    "        \n",
    "        len_gamma_star_ref = len(gamma_star[ref-1])   #cw\n",
    "        gamma_star[ref-1] = [0] * len_gamma_star_ref #cw\n",
    "        delta_star[ref-1] = [1] * len_gamma_star_ref # cw\n",
    "        \n",
    "\n",
    "    return(gamma_star, delta_star, batch_design)\n",
    "\n",
    "\n",
    "def adjust_data(s_data, gamma_star, delta_star, batch_design, n_batches, var_pooled, stand_mean, n_array, ref, batches, dat):\n",
    "    \"\"\"Adjust the data -- corrects for estimated batch effects\n",
    "\n",
    "    Arguments:\n",
    "        s_data {matrix} -- standardised data matrix\n",
    "        gamma_star {matrix} -- estimated additive batch effect\n",
    "        delta_star {matrix} -- estimated multiplicative batch effect\n",
    "        batch_design {matrix} -- information about batches in design matrix\n",
    "        n_batches {int list} -- list of batches lengths\n",
    "        stand_mean {matrix} -- standardised mean\n",
    "        var_pooled {matrix} -- Variance for each gene and each batch\n",
    "        n_array {int} -- total size of dataset\n",
    "        ref {int} -- the index of the reference batch in the batch list\n",
    "        batches {int list} -- list of unique batches\n",
    "        dat\n",
    "\n",
    "    Returns:\n",
    "        bayes_data [matrix] -- data adjusted for correction of batch effects\n",
    "    \"\"\"\n",
    "    # Now we adjust the data:\n",
    "    # 1. substract additive batch effect (gamma_star)\n",
    "    # 2. divide by multiplicative batch effect (delta_star)\n",
    "    print(\"Adjusting the Data\")\n",
    "    #bayes_data = np.transpose(s_data)\n",
    "    bayes_data = s_data\n",
    "    j = 0   #cw\n",
    "    for i in batches:\n",
    "        bayes_data[:, i] = (bayes_data[:, i] - np.transpose(np.dot(np.transpose(batch_design)[i, :], gamma_star))) / \\\n",
    "                        (np.dot(np.sqrt(delta_star[j, :]).reshape(data.shape[0], 1), np.repeat(1, n_batches[j]).reshape(1, n_batches[j])))\n",
    "        j += 1\n",
    "   \n",
    "        \n",
    "    # renormalise the data after correction:\n",
    "    # 1. multiply by variance\n",
    "    # 2. add mean\n",
    "    #bayes_data = np.multiply(np.transpose(bayes_data), np.outer(\n",
    "    #    np.sqrt(var_pooled), np.asarray([1]*n_array))) + stand_mean\n",
    "    bayes_data = np.multiply(bayes_data, np.dot(np.sqrt(var_pooled.reshape(len(bayes_data), 1)), np.repeat(1, n_array).reshape(1, n_array))) + stand_mean #cw\n",
    "    # correction for reference batch\n",
    "    if ref is not None:\n",
    "        #bayes_data[batches[ref]] = dat[batches[ref]]\n",
    "        #bayes_data[batches[ref-1]] = dat[batches[ref-1]]  # cw\n",
    "         bayes_data[:,batches[ref-1]] = dat[:, batches[ref-1]]\n",
    "    \n",
    "    # returns the data corrected for batch effects\n",
    "    return bayes_data\n",
    "\n",
    "\n",
    "\n",
    "def pycombat(data, batch, mod=[], par_prior=True, prior_plots=False, mean_only=False, ref_batch=None, precision=None, **kwargs):\n",
    "    \"\"\"Corrects batch effect in microarray expression data. Takes an gene expression file and a list of known batches corresponding to each sample.\n",
    "\n",
    "    Arguments:\n",
    "        data {matrix} -- The expression matrix (dataframe). It contains the information about the gene expression (rows) for each sample (columns).\n",
    "\n",
    "        batch {list} -- List of batch indexes. The batch list describes the batch for each sample. The batches list has as many elements as the number of columns in the expression matrix.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        mod {list} -- List (or list of lists) of covariate(s) indexes. The mod list describes the covariate(s) for each sample. Each mod list has as many elements as the number of columns in the expression matrix (default: {[]}).\n",
    "\n",
    "        par_prior {bool} -- False for non-parametric estimation of batch effects (default: {True}).\n",
    "\n",
    "        prior_plots {bool} -- True if requires to plot the priors (default: {False} -- Not implemented yet!).\n",
    "\n",
    "        mean_only {bool} -- True iff just adjusting the means and not individual batch effects (default: {False}).\n",
    "\n",
    "        ref_batch -- reference batch selected (default: {None}).\n",
    "\n",
    "        precision {float} -- level of precision for precision computing (default: {None}).\n",
    "\n",
    "    Returns:\n",
    "        bayes_data_df -- The expression dataframe adjusted for batch effects.\n",
    "    \"\"\"\n",
    "\n",
    "    list_samples = data.columns\n",
    "    list_genes = data.index\n",
    "    dat = data.values\n",
    "\n",
    "    check_mean_only(mean_only)\n",
    "\n",
    "    batchmod = define_batchmod(batch)\n",
    "    ref, batchmod = check_ref_batch(ref_batch, batch, batchmod)\n",
    "    n_batch, batches, n_batches, n_array = treat_batches(batch)\n",
    "    design = treat_covariates(batchmod, mod, ref, n_batch)\n",
    "    NAs = check_NAs(dat)\n",
    "    if not(NAs):\n",
    "        B_hat, grand_mean, var_pooled = calculate_mean_var(\n",
    "            design, batches, ref, dat, NAs, n_batches, n_batch, n_array)\n",
    "        stand_mean = calculate_stand_mean(\n",
    "            grand_mean, n_array, design, n_batch, B_hat)\n",
    "        s_data = standardise_data(dat, stand_mean, var_pooled, n_array)\n",
    "        gamma_star, delta_star, batch_design = fit_model(\n",
    "            design, n_batch, s_data, batches, mean_only, par_prior, precision, ref, NAs)\n",
    "        bayes_data = adjust_data(s_data, gamma_star, delta_star, batch_design,\n",
    "                                n_batches, var_pooled, stand_mean, n_array, ref, batches, dat)\n",
    "\n",
    "        bayes_data_df = pd.DataFrame(bayes_data,\n",
    "                    columns = list_samples,\n",
    "                    index = list_genes)\n",
    "\n",
    "        return(bayes_data_df)\n",
    "    else:\n",
    "        raise ValueError(\"NaN value is not accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71676590-5d55-423e-a032-daa75a85df19",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mydata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22960\\2106514157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Read data CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mydata.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Display the contents of the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mydata.csv'"
     ]
    }
   ],
   "source": [
    "# example\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data CSV file\n",
    "data = pd.read_csv('mydata.csv',index_col=0)\n",
    "\n",
    "# Display the contents of the DataFrame\n",
    "# plt.boxplot(data)\n",
    "# plt.show()\n",
    "#type(data)\n",
    "#print(data.shape)\n",
    "\n",
    "# Read batch and mod1 CSV file\n",
    "batch = pd.read_csv('batch.csv')\n",
    "mod = pd.read_csv('mod1.csv')\n",
    "# Extract batch values as a list\n",
    "batch = batch['x'].tolist()\n",
    "mod = mod['x'].tolist()\n",
    "# Display the batch list\n",
    "print(batch)\n",
    "type(batch)\n",
    "print(mod)\n",
    "type(mod)\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bdfb159-8c58-412b-b50d-3582110e2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pycombat(data, batch, mod=[], par_prior=True, prior_plots=False, mean_only=False, ref_batch=None, precision=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c5daa91-6a9e-4ce1-908a-1d1d7b692138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test the code line by line\n",
    "# prior parameters in the function \n",
    "data = data\n",
    "batch = batch \n",
    "# mod=mod \n",
    "mod=[] \n",
    "par_prior=True\n",
    "prior_plots=False\n",
    "mean_only=False \n",
    "ref_batch= 5\n",
    "precision=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5872506-08bc-404d-8827-d48674b929ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch 5 as a reference batch.\n",
      "Found 5 batches.\n",
      "Found 5 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n",
      "Adjusting the Data\n",
      "Adjusting the Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 9.83888669,  7.9761972 ,  8.14654578, ...,  9.63194369,\n",
       "          9.67037063,  9.17899055],\n",
       "        [ 6.11814681,  5.33793738,  5.38030388, ...,  5.81848972,\n",
       "          5.78331765,  5.83190232],\n",
       "        [ 6.80941935,  6.02823105,  5.91419584, ...,  5.9940378 ,\n",
       "          6.18881758,  6.49504447],\n",
       "        ...,\n",
       "        [ 9.45778905,  8.76936447,  8.9712509 , ...,  9.97459153,\n",
       "          9.9648727 ,  8.90834413],\n",
       "        [ 9.60548695,  9.49969345,  9.40407306, ..., 10.56150332,\n",
       "         10.53960702,  9.77760834],\n",
       "        [11.65264685, 11.75798796, 11.48257458, ..., 12.39749555,\n",
       "         12.34127524, 11.97825141]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data\n",
    "list_samples = data.columns\n",
    "list_samples\n",
    "list_genes = data.index\n",
    "list_genes\n",
    "dat = data.values\n",
    "dat\n",
    "check_mean_only(mean_only)\n",
    "mean_only\n",
    "batchmod = define_batchmod(batch)\n",
    "batchmod\n",
    "ref, batchmod = check_ref_batch(ref_batch, batch, batchmod)\n",
    "ref\n",
    "batchmod\n",
    "ref\n",
    "treat_batches(batch)\n",
    "n_batch, batches, n_batches, n_array = treat_batches(batch)\n",
    "n_batch\n",
    "batches\n",
    "n_array\n",
    "design = treat_covariates(batchmod, mod, ref, n_batch)\n",
    "design\n",
    "NAs = check_NAs(dat)\n",
    "NAs\n",
    "B_hat, grand_mean, var_pooled = calculate_mean_var(\n",
    "           design, batches, ref, dat, NAs, n_batches, n_batch, n_array)\n",
    "B_hat\n",
    "grand_mean\n",
    "var_pooled\n",
    "stand_mean = calculate_stand_mean(\n",
    "           grand_mean, n_array, design, n_batch, B_hat)\n",
    "stand_mean\n",
    "\n",
    "s_data = standardise_data(dat, stand_mean, var_pooled, n_array)\n",
    "s_data\n",
    "\n",
    "gamma_star, delta_star, batch_design = fit_model(\n",
    "           design, n_batch, s_data, batches, mean_only, par_prior, precision, ref, NAs)\n",
    "gamma_star\n",
    "delta_star\n",
    "batch_design\n",
    "\n",
    "adjust_data(s_data, gamma_star, delta_star, batch_design,\n",
    "                               n_batches, var_pooled, stand_mean, n_array, ref, batches, dat)\n",
    "bayes_data = adjust_data(s_data, gamma_star, delta_star, batch_design,\n",
    "                               n_batches, var_pooled, stand_mean, n_array, ref, batches, dat)\n",
    "bayes_data\n",
    "bayes_data_df = pd.DataFrame(bayes_data,\n",
    "                   columns = list_samples,\n",
    "                  index = list_genes)\n",
    "bayes_data_df\n",
    "bayes_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
